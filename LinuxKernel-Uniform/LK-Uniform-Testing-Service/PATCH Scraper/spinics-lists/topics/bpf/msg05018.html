<!-- MHonArc v2.6.19 -->
<!--X-Subject: [PATCH v5 bpf&#45;next 1/5] libbpf: add perf buffer API -->
<!--X-From-R13: Oaqevv @nxelvxb &#60;naqevvaNso.pbz> -->
<!--X-Date: Fri, 5 Jul 2019 21:35:37 &#45;0700 -->
<!--X-Message-Id: 20190706043522.1559005&#45;2&#45;andriin@fb.com -->
<!--X-Content-Type: text/plain -->
<!--X-Reference: 20190706043522.1559005&#45;1&#45;andriin@fb.com -->
<!--X-Head-End-->
<!doctype html public "-//W3C//DTD HTML//EN">
<html>
<head>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({
          google_ad_client: "ca-pub-3422782820843221",
          enable_page_level_ads: true
     });
</script>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="BPF: [PATCH v5 bpf-next 1/5] libbpf: add perf buffer API">
<style>
<!--
 pre {white-space: pre-wrap;}
-->
</style>
<title>[PATCH v5 bpf-next 1/5] libbpf: add perf buffer API &mdash; BPF</title>
<link rel="alternate" type="application/rss+xml" title="BPF" href="//feeds.feedburner.com/packetfilters">
<script type="text/javascript">
var addthis_config = addthis_config||{};
addthis_config.data_track_addressbar = false;
</script>
</head>
<body itemscope itemtype="//schema.org/Article" bgcolor=white vlink=green link=blue>
<!--X-Body-Begin-->
<!--X-User-Header-->
<!--X-User-Header-End-->
<!--X-TopPNI-->
<form action="//www.google.com" id="cse-search-box" target="_blank">
  <div>
    <input type="hidden" name="cx" value="partner-pub-3422782820843221:9580497365" />
    <input type="hidden" name="ie" value="UTF-8" />
    <input type="text" name="q" size="25" />
    <input type="submit" name="sa" value="Search" />
  </div>
</form>
<script type="text/javascript" src="//www.google.com/coop/cse/brand?form=cse-search-box&amp;lang=en"></script>
<h1 itemprop="name">[PATCH v5 bpf-next 1/5] libbpf: add perf buffer API</h1>
[<a href="msg05017.html">Date Prev</a>][<a href="msg05019.html">Date Next</a>][<a href="msg05017.html">Thread Prev</a>][<a href="msg05025.html">Thread Next</a>][<a href="maillist.html#05018">Date Index</a>][<a href="index.html#05018">Thread Index</a>]


<p>&nbsp;<br>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- responsive test for archives -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-3422782820843221"
     data-ad-slot="6345952567"
     data-ad-format="auto"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>

<!--X-TopPNI-End-->
<!--X-MsgBody-->
<!--X-Subject-Header-Begin-->
<hr>
<!--X-Subject-Header-End-->
<!--X-Head-of-Message-->
<ul>
<li><em>Subject</em>: [PATCH v5 bpf-next 1/5] libbpf: add perf buffer API</li>
<li><em>From</em>: Andrii Nakryiko &lt;andriin@xxxxxx&gt;</li>
<li><em>Date</em>: Fri, 5 Jul 2019 21:35:18 -0700</li>
<li><em>Cc</em>: Andrii Nakryiko &lt;andriin@xxxxxx&gt;</li>
<li><em>In-reply-to</em>: &lt;<a href="msg05017.html">20190706043522.1559005-1-andriin@fb.com</a>&gt;</li>
<li><em>Smtp-origin-cluster</em>: prn2c23</li>
<li><em>Smtp-origin-hostname</em>: dev101.prn2.facebook.com</li>
<li><em>Smtp-origin-hostprefix</em>: dev</li>
</ul>
<!--X-Head-of-Message-End-->
<!--X-Head-Body-Sep-Begin-->
<!-- AddThis Button BEGIN -->
<div class="addthis_toolbox addthis_default_style ">
<a class="addthis_button_preferred_1"></a>
<a class="addthis_button_preferred_2"></a>
<a class="addthis_button_preferred_3"></a>
<a class="addthis_button_preferred_4"></a>
<a class="addthis_button_compact"></a>
<a class="addthis_counter addthis_bubble_style"></a>
</div>
<script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5196c2ae1be43d18&async=1&domready=1" async></script>
<!-- AddThis Button END -->
<hr>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- responsive link 1 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-3422782820843221"
     data-ad-slot="8681825769"
     data-ad-format="link"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<p>
<div class="content" itemprop="articleBody">
<!--X-Head-Body-Sep-End-->
<!--X-Body-of-Message-->
<pre>BPF_MAP_TYPE_PERF_EVENT_ARRAY map is often used to send data from BPF program
to user space for additional processing. libbpf already has very low-level API
to read single CPU perf buffer, bpf_perf_event_read_simple(), but it's hard to
use and requires a lot of code to set everything up. This patch adds
perf_buffer abstraction on top of it, abstracting setting up and polling
per-CPU logic into simple and convenient API, similar to what BCC provides.

perf_buffer__new() sets up per-CPU ring buffers and updates corresponding BPF
map entries. It accepts two user-provided callbacks: one for handling raw
samples and one for get notifications of lost samples due to buffer overflow.

perf_buffer__new_raw() is similar, but provides more control over how
perf events are set up (by accepting user-provided perf_event_attr), how
they are handled (perf_event_header pointer is passed directly to
user-provided callback), and on which CPUs ring buffers are created
(it's possible to provide a list of CPUs and corresponding map keys to
update). This API allows advanced users fuller control.

perf_buffer__poll() is used to fetch ring buffer data across all CPUs,
utilizing epoll instance.

perf_buffer__free() does corresponding clean up and unsets FDs from BPF map.

All APIs are not thread-safe. User should ensure proper locking/coordination if
used in multi-threaded set up.

Signed-off-by: Andrii Nakryiko &lt;andriin@xxxxxx&gt;
---
 tools/lib/bpf/libbpf.c   | 366 +++++++++++++++++++++++++++++++++++++++
 tools/lib/bpf/libbpf.h   |  49 ++++++
 tools/lib/bpf/libbpf.map |   4 +
 3 files changed, 419 insertions(+)

diff --git a/tools/lib/bpf/libbpf.c b/tools/lib/bpf/libbpf.c
index 2a08eb106221..72149d68b8c1 100644
--- a/tools/lib/bpf/libbpf.c
+++ b/tools/lib/bpf/libbpf.c
@@ -32,7 +32,9 @@
 #include &lt;linux/limits.h&gt;
 #include &lt;linux/perf_event.h&gt;
 #include &lt;linux/ring_buffer.h&gt;
+#include &lt;sys/epoll.h&gt;
 #include &lt;sys/ioctl.h&gt;
+#include &lt;sys/mman.h&gt;
 #include &lt;sys/stat.h&gt;
 #include &lt;sys/types.h&gt;
 #include &lt;sys/vfs.h&gt;
@@ -4354,6 +4356,370 @@ bpf_perf_event_read_simple(void *mmap_mem, size_t mmap_size, size_t page_size,
 	return ret;
 }
 
+struct perf_buffer;
+
+struct perf_buffer_params {
+	struct perf_event_attr *attr;
+	/* if event_cb is specified, it takes precendence */
+	perf_buffer_event_fn event_cb;
+	/* sample_cb and lost_cb are higher-level common-case callbacks */
+	perf_buffer_sample_fn sample_cb;
+	perf_buffer_lost_fn lost_cb;
+	void *ctx;
+	int cpu_cnt;
+	int *cpus;
+	int *map_keys;
+};
+
+struct perf_cpu_buf {
+	struct perf_buffer *pb;
+	void *base; /* mmap()'ed memory */
+	void *buf; /* for reconstructing segmented data */
+	size_t buf_size;
+	int fd;
+	int cpu;
+	int map_key;
+};
+
+struct perf_buffer {
+	perf_buffer_event_fn event_cb;
+	perf_buffer_sample_fn sample_cb;
+	perf_buffer_lost_fn lost_cb;
+	void *ctx; /* passed into callbacks */
+
+	size_t page_size;
+	size_t mmap_size;
+	struct perf_cpu_buf **cpu_bufs;
+	struct epoll_event *events;
+	int cpu_cnt;
+	int epoll_fd; /* perf event FD */
+	int map_fd; /* BPF_MAP_TYPE_PERF_EVENT_ARRAY BPF map FD */
+};
+
+static void perf_buffer__free_cpu_buf(struct perf_buffer *pb,
+				      struct perf_cpu_buf *cpu_buf)
+{
+	if (!cpu_buf)
+		return;
+	if (cpu_buf-&gt;base &amp;&amp;
+	    munmap(cpu_buf-&gt;base, pb-&gt;mmap_size + pb-&gt;page_size))
+		pr_warning(&quot;failed to munmap cpu_buf #%d\n&quot;, cpu_buf-&gt;cpu);
+	if (cpu_buf-&gt;fd &gt;= 0) {
+		ioctl(cpu_buf-&gt;fd, PERF_EVENT_IOC_DISABLE, 0);
+		close(cpu_buf-&gt;fd);
+	}
+	free(cpu_buf-&gt;buf);
+	free(cpu_buf);
+}
+
+void perf_buffer__free(struct perf_buffer *pb)
+{
+	int i;
+
+	if (!pb)
+		return;
+	if (pb-&gt;cpu_bufs) {
+		for (i = 0; i &lt; pb-&gt;cpu_cnt &amp;&amp; pb-&gt;cpu_bufs[i]; i++) {
+			struct perf_cpu_buf *cpu_buf = pb-&gt;cpu_bufs[i];
+
+			bpf_map_delete_elem(pb-&gt;map_fd, &amp;cpu_buf-&gt;map_key);
+			perf_buffer__free_cpu_buf(pb, cpu_buf);
+		}
+		free(pb-&gt;cpu_bufs);
+	}
+	if (pb-&gt;epoll_fd &gt;= 0)
+		close(pb-&gt;epoll_fd);
+	free(pb-&gt;events);
+	free(pb);
+}
+
+static struct perf_cpu_buf *
+perf_buffer__open_cpu_buf(struct perf_buffer *pb, struct perf_event_attr *attr,
+			  int cpu, int map_key)
+{
+	struct perf_cpu_buf *cpu_buf;
+	char msg[STRERR_BUFSIZE];
+	int err;
+
+	cpu_buf = calloc(1, sizeof(*cpu_buf));
+	if (!cpu_buf)
+		return ERR_PTR(-ENOMEM);
+
+	cpu_buf-&gt;pb = pb;
+	cpu_buf-&gt;cpu = cpu;
+	cpu_buf-&gt;map_key = map_key;
+
+	cpu_buf-&gt;fd = syscall(__NR_perf_event_open, attr, -1 /* pid */, cpu,
+			      -1, PERF_FLAG_FD_CLOEXEC);
+	if (cpu_buf-&gt;fd &lt; 0) {
+		err = -errno;
+		pr_warning(&quot;failed to open perf buffer event on cpu #%d: %s\n&quot;,
+			   cpu, libbpf_strerror_r(err, msg, sizeof(msg)));
+		goto error;
+	}
+
+	cpu_buf-&gt;base = mmap(NULL, pb-&gt;mmap_size + pb-&gt;page_size,
+			     PROT_READ | PROT_WRITE, MAP_SHARED,
+			     cpu_buf-&gt;fd, 0);
+	if (cpu_buf-&gt;base == MAP_FAILED) {
+		cpu_buf-&gt;base = NULL;
+		err = -errno;
+		pr_warning(&quot;failed to mmap perf buffer on cpu #%d: %s\n&quot;,
+			   cpu, libbpf_strerror_r(err, msg, sizeof(msg)));
+		goto error;
+	}
+
+	if (ioctl(cpu_buf-&gt;fd, PERF_EVENT_IOC_ENABLE, 0) &lt; 0) {
+		err = -errno;
+		pr_warning(&quot;failed to enable perf buffer event on cpu #%d: %s\n&quot;,
+			   cpu, libbpf_strerror_r(err, msg, sizeof(msg)));
+		goto error;
+	}
+
+	return cpu_buf;
+
+error:
+	perf_buffer__free_cpu_buf(pb, cpu_buf);
+	return (struct perf_cpu_buf *)ERR_PTR(err);
+}
+
+static struct perf_buffer *__perf_buffer__new(int map_fd, size_t page_cnt,
+					      struct perf_buffer_params *p);
+
+struct perf_buffer *perf_buffer__new(int map_fd, size_t page_cnt,
+				     const struct perf_buffer_opts *opts)
+{
+	struct perf_buffer_params p = {};
+	struct perf_event_attr attr = {
+		.config = PERF_COUNT_SW_BPF_OUTPUT,
+		.type = PERF_TYPE_SOFTWARE,
+		.sample_type = PERF_SAMPLE_RAW,
+		.sample_period = 1,
+		.wakeup_events = 1,
+	};
+
+	p.attr = &amp;attr;
+	p.sample_cb = opts ? opts-&gt;sample_cb : NULL;
+	p.lost_cb = opts ? opts-&gt;lost_cb : NULL;
+	p.ctx = opts ? opts-&gt;ctx : NULL;
+
+	return __perf_buffer__new(map_fd, page_cnt, &amp;p);
+}
+
+struct perf_buffer *
+perf_buffer__new_raw(int map_fd, size_t page_cnt,
+		     const struct perf_buffer_raw_opts *opts)
+{
+	struct perf_buffer_params p = {};
+
+	p.attr = opts-&gt;attr;
+	p.event_cb = opts-&gt;event_cb;
+	p.ctx = opts-&gt;ctx;
+	p.cpu_cnt = opts-&gt;cpu_cnt;
+	p.cpus = opts-&gt;cpus;
+	p.map_keys = opts-&gt;map_keys;
+
+	return __perf_buffer__new(map_fd, page_cnt, &amp;p);
+}
+
+static struct perf_buffer *__perf_buffer__new(int map_fd, size_t page_cnt,
+					      struct perf_buffer_params *p)
+{
+	struct bpf_map_info map = {};
+	char msg[STRERR_BUFSIZE];
+	struct perf_buffer *pb;
+	__u32 map_info_len;
+	int err, i;
+
+	if (page_cnt &amp; (page_cnt - 1)) {
+		pr_warning(&quot;page count should be power of two, but is %zu\n&quot;,
+			   page_cnt);
+		return ERR_PTR(-EINVAL);
+	}
+
+	map_info_len = sizeof(map);
+	err = bpf_obj_get_info_by_fd(map_fd, &amp;map, &amp;map_info_len);
+	if (err) {
+		err = -errno;
+		pr_warning(&quot;failed to get map info for map FD %d: %s\n&quot;,
+			   map_fd, libbpf_strerror_r(err, msg, sizeof(msg)));
+		return ERR_PTR(err);
+	}
+
+	if (map.type != BPF_MAP_TYPE_PERF_EVENT_ARRAY) {
+		pr_warning(&quot;map '%s' should be BPF_MAP_TYPE_PERF_EVENT_ARRAY\n&quot;,
+			   map.name);
+		return ERR_PTR(-EINVAL);
+	}
+
+	pb = calloc(1, sizeof(*pb));
+	if (!pb)
+		return ERR_PTR(-ENOMEM);
+
+	pb-&gt;event_cb = p-&gt;event_cb;
+	pb-&gt;sample_cb = p-&gt;sample_cb;
+	pb-&gt;lost_cb = p-&gt;lost_cb;
+	pb-&gt;ctx = p-&gt;ctx;
+
+	pb-&gt;page_size = getpagesize();
+	pb-&gt;mmap_size = pb-&gt;page_size * page_cnt;
+	pb-&gt;map_fd = map_fd;
+
+	pb-&gt;epoll_fd = epoll_create1(EPOLL_CLOEXEC);
+	if (pb-&gt;epoll_fd &lt; 0) {
+		err = -errno;
+		pr_warning(&quot;failed to create epoll instance: %s\n&quot;,
+			   libbpf_strerror_r(err, msg, sizeof(msg)));
+		goto error;
+	}
+
+	if (p-&gt;cpu_cnt &gt; 0) {
+		pb-&gt;cpu_cnt = p-&gt;cpu_cnt;
+	} else {
+		pb-&gt;cpu_cnt = libbpf_num_possible_cpus();
+		if (pb-&gt;cpu_cnt &lt; 0) {
+			err = pb-&gt;cpu_cnt;
+			goto error;
+		}
+		if (map.max_entries &lt; pb-&gt;cpu_cnt)
+			pb-&gt;cpu_cnt = map.max_entries;
+	}
+
+	pb-&gt;events = calloc(pb-&gt;cpu_cnt, sizeof(*pb-&gt;events));
+	if (!pb-&gt;events) {
+		err = -ENOMEM;
+		pr_warning(&quot;failed to allocate events: out of memory\n&quot;);
+		goto error;
+	}
+	pb-&gt;cpu_bufs = calloc(pb-&gt;cpu_cnt, sizeof(*pb-&gt;cpu_bufs));
+	if (!pb-&gt;cpu_bufs) {
+		err = -ENOMEM;
+		pr_warning(&quot;failed to allocate buffers: out of memory\n&quot;);
+		goto error;
+	}
+
+	for (i = 0; i &lt; pb-&gt;cpu_cnt; i++) {
+		struct perf_cpu_buf *cpu_buf;
+		int cpu, map_key;
+
+		cpu = p-&gt;cpu_cnt &gt; 0 ? p-&gt;cpus[i] : i;
+		map_key = p-&gt;cpu_cnt &gt; 0 ? p-&gt;map_keys[i] : i;
+
+		cpu_buf = perf_buffer__open_cpu_buf(pb, p-&gt;attr, cpu, map_key);
+		if (IS_ERR(cpu_buf)) {
+			err = PTR_ERR(cpu_buf);
+			goto error;
+		}
+
+		pb-&gt;cpu_bufs[i] = cpu_buf;
+
+		err = bpf_map_update_elem(pb-&gt;map_fd, &amp;map_key,
+					  &amp;cpu_buf-&gt;fd, 0);
+		if (err) {
+			err = -errno;
+			pr_warning(&quot;failed to set cpu #%d, key %d -&gt; perf FD %d: %s\n&quot;,
+				   cpu, map_key, cpu_buf-&gt;fd,
+				   libbpf_strerror_r(err, msg, sizeof(msg)));
+			goto error;
+		}
+
+		pb-&gt;events[i].events = EPOLLIN;
+		pb-&gt;events[i].data.ptr = cpu_buf;
+		if (epoll_ctl(pb-&gt;epoll_fd, EPOLL_CTL_ADD, cpu_buf-&gt;fd,
+			      &amp;pb-&gt;events[i]) &lt; 0) {
+			err = -errno;
+			pr_warning(&quot;failed to epoll_ctl cpu #%d perf FD %d: %s\n&quot;,
+				   cpu, cpu_buf-&gt;fd,
+				   libbpf_strerror_r(err, msg, sizeof(msg)));
+			goto error;
+		}
+	}
+
+	return pb;
+
+error:
+	if (pb)
+		perf_buffer__free(pb);
+	return ERR_PTR(err);
+}
+
+struct perf_sample_raw {
+	struct perf_event_header header;
+	uint32_t size;
+	char data[0];
+};
+
+struct perf_sample_lost {
+	struct perf_event_header header;
+	uint64_t id;
+	uint64_t lost;
+	uint64_t sample_id;
+};
+
+static enum bpf_perf_event_ret
+perf_buffer__process_record(struct perf_event_header *e, void *ctx)
+{
+	struct perf_cpu_buf *cpu_buf = ctx;
+	struct perf_buffer *pb = cpu_buf-&gt;pb;
+	void *data = e;
+
+	/* user wants full control over parsing perf event */
+	if (pb-&gt;event_cb)
+		return pb-&gt;event_cb(pb-&gt;ctx, cpu_buf-&gt;cpu, e);
+
+	switch (e-&gt;type) {
+	case PERF_RECORD_SAMPLE: {
+		struct perf_sample_raw *s = data;
+
+		if (pb-&gt;sample_cb)
+			pb-&gt;sample_cb(pb-&gt;ctx, cpu_buf-&gt;cpu, s-&gt;data, s-&gt;size);
+		break;
+	}
+	case PERF_RECORD_LOST: {
+		struct perf_sample_lost *s = data;
+
+		if (pb-&gt;lost_cb)
+			pb-&gt;lost_cb(pb-&gt;ctx, cpu_buf-&gt;cpu, s-&gt;lost);
+		break;
+	}
+	default:
+		pr_warning(&quot;unknown perf sample type %d\n&quot;, e-&gt;type);
+		return LIBBPF_PERF_EVENT_ERROR;
+	}
+	return LIBBPF_PERF_EVENT_CONT;
+}
+
+static int perf_buffer__process_records(struct perf_buffer *pb,
+					struct perf_cpu_buf *cpu_buf)
+{
+	enum bpf_perf_event_ret ret;
+
+	ret = bpf_perf_event_read_simple(cpu_buf-&gt;base, pb-&gt;mmap_size,
+					 pb-&gt;page_size, &amp;cpu_buf-&gt;buf,
+					 &amp;cpu_buf-&gt;buf_size,
+					 perf_buffer__process_record, cpu_buf);
+	if (ret != LIBBPF_PERF_EVENT_CONT)
+		return ret;
+	return 0;
+}
+
+int perf_buffer__poll(struct perf_buffer *pb, int timeout_ms)
+{
+	int cnt, err;
+
+	cnt = epoll_wait(pb-&gt;epoll_fd, pb-&gt;events, pb-&gt;cpu_cnt, timeout_ms);
+	for (int i = 0; i &lt; cnt; i++) {
+		struct perf_cpu_buf *cpu_buf = pb-&gt;events[i].data.ptr;
+
+		err = perf_buffer__process_records(pb, cpu_buf);
+		if (err) {
+			pr_warning(&quot;error while processing records: %d\n&quot;, err);
+			return err;
+		}
+	}
+	return cnt &lt; 0 ? -errno : cnt;
+}
+
 struct bpf_prog_info_array_desc {
 	int	array_offset;	/* e.g. offset of jited_prog_insns */
 	int	count_offset;	/* e.g. offset of jited_prog_len */
diff --git a/tools/lib/bpf/libbpf.h b/tools/lib/bpf/libbpf.h
index f55933784f95..5cbf459ece0b 100644
--- a/tools/lib/bpf/libbpf.h
+++ b/tools/lib/bpf/libbpf.h
@@ -358,6 +358,26 @@ LIBBPF_API int bpf_prog_load(const char *file, enum bpf_prog_type type,
 LIBBPF_API int bpf_set_link_xdp_fd(int ifindex, int fd, __u32 flags);
 LIBBPF_API int bpf_get_link_xdp_id(int ifindex, __u32 *prog_id, __u32 flags);
 
+struct perf_buffer;
+
+typedef void (*perf_buffer_sample_fn)(void *ctx, int cpu,
+				      void *data, __u32 size);
+typedef void (*perf_buffer_lost_fn)(void *ctx, int cpu, __u64 cnt);
+
+/* common use perf buffer options */
+struct perf_buffer_opts {
+	/* if specified, sample_cb is called for each sample */
+	perf_buffer_sample_fn sample_cb;
+	/* if specified, lost_cb is called for each batch of lost samples */
+	perf_buffer_lost_fn lost_cb;
+	/* ctx is provided to sample_cb and lost_cb */
+	void *ctx;
+};
+
+LIBBPF_API struct perf_buffer *
+perf_buffer__new(int map_fd, size_t page_cnt,
+		 const struct perf_buffer_opts *opts);
+
 enum bpf_perf_event_ret {
 	LIBBPF_PERF_EVENT_DONE	= 0,
 	LIBBPF_PERF_EVENT_ERROR	= -1,
@@ -365,6 +385,35 @@ enum bpf_perf_event_ret {
 };
 
 struct perf_event_header;
+
+typedef enum bpf_perf_event_ret
+(*perf_buffer_event_fn)(void *ctx, int cpu, struct perf_event_header *event);
+
+/* raw perf buffer options, giving most power and control */
+struct perf_buffer_raw_opts {
+	/* perf event attrs passed directly into perf_event_open() */
+	struct perf_event_attr *attr;
+	/* raw event callback */
+	perf_buffer_event_fn event_cb;
+	/* ctx is provided to event_cb */
+	void *ctx;
+	/* if cpu_cnt == 0, open all on all possible CPUs (up to the number of
+	 * max_entries of given PERF_EVENT_ARRAY map)
+	 */
+	int cpu_cnt;
+	/* if cpu_cnt &gt; 0, cpus is an array of CPUs to open ring buffers on */
+	int *cpus;
+	/* if cpu_cnt &gt; 0, map_keys specify map keys to set per-CPU FDs for */
+	int *map_keys;
+};
+
+LIBBPF_API struct perf_buffer *
+perf_buffer__new_raw(int map_fd, size_t page_cnt,
+		     const struct perf_buffer_raw_opts *opts);
+
+LIBBPF_API void perf_buffer__free(struct perf_buffer *pb);
+LIBBPF_API int perf_buffer__poll(struct perf_buffer *pb, int timeout_ms);
+
 typedef enum bpf_perf_event_ret
 	(*bpf_perf_event_print_t)(struct perf_event_header *hdr,
 				  void *private_data);
diff --git a/tools/lib/bpf/libbpf.map b/tools/lib/bpf/libbpf.map
index e6b7d4edbc93..f9d316e873d8 100644
--- a/tools/lib/bpf/libbpf.map
+++ b/tools/lib/bpf/libbpf.map
@@ -179,4 +179,8 @@ LIBBPF_0.0.4 {
 		btf_dump__new;
 		btf__parse_elf;
 		libbpf_num_possible_cpus;
+		perf_buffer__free;
+		perf_buffer__new;
+		perf_buffer__new_raw;
+		perf_buffer__poll;
 } LIBBPF_0.0.3;
-- 
2.17.1



</pre>
<!--X-Body-of-Message-End-->
<!--X-MsgBody-End-->
<!--X-Follow-Ups-->
</div>
<hr>
<ul><li><strong>Follow-Ups</strong>:
<ul>
<li><strong><a name="05025" href="msg05025.html">Re: [PATCH v5 bpf-next 1/5] libbpf: add perf buffer API</a></strong>
<ul><li><em>From:</em> Yonghong Song</li></ul></li>
</ul></li></ul>
<!--X-Follow-Ups-End-->
<!--X-References-->
<ul><li><strong>References</strong>:
<ul>
<li><strong><a name="05017" href="msg05017.html">[PATCH v5 bpf-next 0/5] libbpf: add perf buffer abstraction and API</a></strong>
<ul><li><em>From:</em> Andrii Nakryiko</li></ul></li>
</ul></li></ul>
<!--X-References-End-->
<!--X-BotPNI-->
<ul>
<li>Prev by Date:
<strong><a href="msg05017.html">[PATCH v5 bpf-next 0/5] libbpf: add perf buffer abstraction and API</a></strong>
</li>
<li>Next by Date:
<strong><a href="msg05019.html">[PATCH v5 bpf-next 2/5] libbpf: auto-set PERF_EVENT_ARRAY size to number of CPUs</a></strong>
</li>
<li>Previous by thread:
<strong><a href="msg05017.html">[PATCH v5 bpf-next 0/5] libbpf: add perf buffer abstraction and API</a></strong>
</li>
<li>Next by thread:
<strong><a href="msg05025.html">Re: [PATCH v5 bpf-next 1/5] libbpf: add perf buffer API</a></strong>
</li>
<li>Index(es):
<ul>
<li><a href="maillist.html#05018"><strong>Date</strong></a></li>
<li><a href="index.html#05018"><strong>Thread</strong></a></li>
</ul>
</li>
</ul>

<!--X-BotPNI-End-->
<!--X-User-Footer-->
<center>
<font size=-1>
<a href=/lists/>[Index&nbsp;of&nbsp;Archives]</a>
&nbsp;
&nbsp;
<a href=/lists/linux-samsung-soc/>[Linux&nbsp;Samsung&nbsp;SoC]</a>
&nbsp;
&nbsp;
<a href=/lists/linux-rockchip/>[Linux&nbsp;Rockchip&nbsp;SoC]</a>
&nbsp;
&nbsp;
<a href=/lists/linux-actions/>[Linux&nbsp;Actions&nbsp;SoC]</a>
&nbsp;
&nbsp;
<a href=/lists/linux-snps-arc/>[Linux&nbsp;for&nbsp;Synopsys&nbsp;ARC&nbsp;Processors]</a>
&nbsp;
&nbsp;
<a href=/lists/linux-nfs/>[Linux&nbsp;NFS]</a>
&nbsp;
&nbsp;
<a href=/lists/linux-nilfs/>[Linux&nbsp;NILFS]</a>
&nbsp;
&nbsp;
<a href=/lists/linux-usb/>[Linux&nbsp;USB&nbsp;Devel]</a>
&nbsp;
&nbsp;
<a href=/lists/vfl/>[Video&nbsp;for&nbsp;Linux]</a>
&nbsp;
&nbsp;
<a href=/lists/linux-audio-users/>[Linux&nbsp;Audio&nbsp;Users]</a>
&nbsp;
&nbsp;
<a href=https://yosemitenews.info/>[Yosemite&nbsp;News]</a>
&nbsp;
&nbsp;
<a href=/lists/kernel/>[Linux&nbsp;Kernel]</a>
&nbsp;
&nbsp;
<a href=/lists/linux-scsi/>[Linux&nbsp;SCSI]</a>
</font>
</center>
<p>
<hr>
<div>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<ins class="adsbygoogle"
     style="display:block"
     data-ad-format="autorelaxed"
     data-ad-client="ca-pub-3422782820843221"
     data-ad-slot="1424524564"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
</div>
<table width=100%>
<tr>
<td align=left>&nbsp;</td>
<td align=right><a href=/lists/><img src=/button_01.gif border=0 alt="Powered by Linux"></a></td>
</tr>
</table>
<!--X-User-Footer-End-->
<script type="text/javascript"> 
 function initAddThis() {
    addthis.init()
 }
initAddThis();
</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-760190-1', 'auto');
  ga('send', 'pageview');
</script>
</body>
</html>
